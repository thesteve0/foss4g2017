== Adding a Database

Now it's time to get to the part of the workshop you have all been waiting for
- getting PostGIS up and running!

=== Project cleanup

We are no longer going to need the NGINX container, so, please clean up your
project from all we have done so far.

[source, bash]
----
$ oc delete all --all
----

=== Generating the Database Pieces

Spinning up our master and replicas is as easy as loading up some JSON directly from GitHub. First we
create some Service Accounts to act on our behalf to carry out cluster tasks with PostGIS.

[source, bash]
----
$ oc create -f https://raw.githubusercontent.com/thesteve0/foss4g2017/master/openshift/set-sa.json
serviceaccount "pgset-sa" created

$ oc policy add-role-to-group view system:serviceaccounts
role "view" added: "system:serviceaccounts"
$ oc policy add-role-to-group edit system:serviceaccounts
role "edit" added: "system:serviceaccounts"
----

Then we go ahead and create databases along with the corresponding services.

[source, bash]
----
$ oc new-app -f https://raw.githubusercontent.com/thesteve0/foss4g2017/master/openshift/database.json

----

That's it - thanks to the work by Jeff McCormick at Crunchy Data Solutions and OpenShift, you now have a master-replica PostGIS
database setup. It may take a while for the images to pull and show up but you can see it in the web overview for the project now.

There is a bug in the interface where it shows the replica service spinning up another pod. Please ignore this, it is
fixed in the new version (3.6).

image::common/3_post_run.png[]


This database is using https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/[StatefulSets] in Kubernetes/OpenShift.
This is a deployment that has extra logic that allows for specialized startup, container order, or data requirements.

=== Loading the Database

Let's go ahead and load up the database in the master with some DDL.

Let's go ahead and shell into the _pgset-0_ pod. Once we do that we can load the data directly from Github using cURL and the psql command.  The simplest way to do this is at the command line:

[source, bash]
----
#shell into the 'master' pod into the 'pg' container
$ oc rsh pgset-0


#now load the data
$ curl https://raw.githubusercontent.com/thesteve0/v3simple-spatial/master/ddl/parkcoord.sql |psql -U postgres -f -  userdb

#now we can load the command line tool to make sure the data is inserted
$ psql -U postgres userdb

userdb=# select count(*) from parkpoints;
 count
--
   547
(1 row)

userdb=# \q
----

You have now loaded your database with a bunch of points for national parks in
the US and Canada. The really amazing part comes next. Go ahead and go to the
overview for the project. Go ahead and click on the circle for the _pgset-replica_
which will bring you to the details page for the pod. Go ahead and click on the terminal tab:

image::common/3_terminal.png[]

In that terminal go ahead and type the following commands:

[source, bash]
----
$ psql -U postgres userdb

userdb=# select count(*) from parkpoints;
 count
--
   547
(1 row)

userdb=# \q

----

Do you REALIZE what just happened. We entered data into the Master DB and it
was automatically replicated over to the replica DB and you did 0 work to make sure
that would happen.

=== Time for More Replication Magic

Let's take this to even another level. In the command line, enter the following command:

[source, bash]
----
 oc patch statefulsets/pgset -p '{"spec":{"replicas":3}}'
----

We just told OpenShift to change the "truth" about the number of replicas. In the web console
you should see the number of replicas increase to 3, 0 is the master, with 1 and 2 being the replicas.

The number inside the _pgset-replica_ circle will increment to 2 and then the blue circle will
fill in the rest of the circle (with that same error still showing). You now have 2 replicas running. If you click
on the circle again you will see the list of the two pods. If you click on the
new pod, the one with the youngest age, and then do the query commands above you will see that it has already been
replicated to the new replica.

In the next section we will spin up Geoserver and connect it to this database. .

<<<
